#!/usr/bin/env python3
"""
Train a PQNetwork from saved training data generated by TrainingDataGenerator.

This script reads one or more .npz files from a given generation data directory
(`training_data/generation_X/data`) that contain:
  - policies: (N, 41)
  - values: (N,)
  - states: (N,) json-serialized GameState strings (bytes in npz â†’ decode utf-8)

It rehydrates GameState objects and calls PQNetwork.train().

Example usage:
  python3 train_pq_from_generation.py \
    --data_dir training_data/generation_0/data \
    --epochs 5 --batch_size 64 \
    --output training_data/models/pq_gen0.keras

Notes:
  - TensorFlow must be installed and available.
  - If multiple .npz files exist, all will be loaded (excluding intermediates).
  - You can limit the number of samples for quick runs with --limit_samples.
"""

import os
import sys
import argparse
import json
import numpy as np
from typing import Dict, List, Tuple

# Ensure we can import from src/
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
SRC_DIR = os.path.join(CURRENT_DIR, 'src')
if SRC_DIR not in sys.path:
    sys.path.insert(0, SRC_DIR)

from core.game import (
    PQNetwork, GameState, CalledSet, Tile, Suit, TileType, TENSORFLOW_AVAILABLE
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train PQNetwork from a generation data directory")
    parser.add_argument("--data_dir", type=str, required=True,
                        help="Path to generation data directory (e.g., training_data/generation_0/data)")
    parser.add_argument("--epochs", type=int, default=5, help="Training epochs")
    parser.add_argument("--batch_size", type=int, default=64, help="Training batch size")
    parser.add_argument("--hidden_size", type=int, default=128, help="PQNetwork hidden size")
    parser.add_argument("--embedding_dim", type=int, default=4, help="Tile embedding dimension")
    parser.add_argument("--max_turns", type=int, default=50, help="Max turns for discard encoding")
    parser.add_argument("--limit_samples", type=int, default=0,
                        help="Optional limit on number of samples to load (0 = no limit)")
    parser.add_argument("--output", type=str, default="",
                        help="Output model filepath (.keras). Default inferred from data_dir")
    return parser.parse_args()


def tile_from_str(tile_str: str) -> Tile:
    """Convert a tile string like '5p' or '3s' into a Tile object."""
    tile_str = str(tile_str)
    tile_type = int(tile_str[:-1])
    suit_char = tile_str[-1]
    suit = Suit.PINZU if suit_char == 'p' else Suit.SOUZU
    return Tile(suit=suit, tile_type=TileType(tile_type))


def rehydrate_called_set(obj: Dict) -> CalledSet:
    tiles = [tile_from_str(t) for t in obj.get('tiles', [])]
    call_type = obj.get('call_type', 'pon')
    called_tile = tile_from_str(obj.get('called_tile')) if obj.get('called_tile') else None
    caller_position = int(obj.get('source_position', 0))  # will be overwritten by pid in rehydrate_state
    source_position = int(obj.get('source_position', 0))
    return CalledSet(tiles=tiles, call_type=call_type, called_tile=called_tile,
                     caller_position=caller_position, source_position=source_position)


def rehydrate_state(json_str: str) -> GameState:
    """Recreate a GameState (and attach optional player_discards attribute) from JSON string."""
    data = json.loads(json_str)

    player_id = int(data.get('player_id', 0))
    player_hand = [tile_from_str(t) for t in data.get('player_hand', [])]
    visible_tiles = [tile_from_str(t) for t in data.get('visible_tiles', [])]
    remaining_tiles = int(data.get('remaining_tiles', 0))

    # Build called_sets mapping
    called_sets_map: Dict[int, List[CalledSet]] = {}
    called_sets_src = data.get('called_sets', {}) or {}
    for pid_str, sets in called_sets_src.items():
        try:
            pid = int(pid_str)
        except Exception:
            pid = int(pid_str) if isinstance(pid_str, int) else 0
        called_sets_list: List[CalledSet] = []
        for cs in sets:
            cs_obj = rehydrate_called_set(cs)
            # Ensure caller_position aligns with pid
            cs_obj.caller_position = pid
            called_sets_list.append(cs_obj)
        called_sets_map[pid] = called_sets_list

    last_discarded_tile = tile_from_str(data['last_discarded_tile']) if data.get('last_discarded_tile') else None
    last_discard_player = int(data['last_discard_player']) if data.get('last_discard_player') is not None else None
    can_call = bool(data.get('can_call', False))

    # other_players_discarded is not used by PQNetwork; create empty lists
    other_players_discarded: Dict[int, List[Tile]] = {i: [] for i in range(4) if i != player_id}

    gs = GameState(
        player_hand=player_hand,
        visible_tiles=visible_tiles,
        remaining_tiles=remaining_tiles,
        player_id=player_id,
        other_players_discarded=other_players_discarded,
        called_sets=called_sets_map,
        last_discarded_tile=last_discarded_tile,
        last_discard_player=last_discard_player,
        can_call=can_call,
    )

    # Attach optional player_discards (list of strings per player) for PQNetwork encoders
    if 'player_discards' in data and isinstance(data['player_discards'], dict):
        try:
            gs.player_discards = {int(k): list(v) for k, v in data['player_discards'].items()}
        except Exception:
            gs.player_discards = {}
    else:
        gs.player_discards = {}

    # If visible_tiles missing or inconsistent, derive from player_discards
    try:
        if (not gs.visible_tiles) and isinstance(gs.player_discards, dict):
            agg: list[Tile] = []
            for i in range(4):
                for t in gs.player_discards.get(i, []):
                    agg.append(tile_from_str(t))
            gs.visible_tiles = agg
    except Exception:
        pass

    return gs


def discover_npz_files(data_dir: str) -> List[str]:
    files = []
    for entry in os.listdir(data_dir):
        if not entry.endswith('.npz'):
            continue
        if 'intermediate' in entry:
            # skip slices; final combined file includes all
            continue
        files.append(os.path.join(data_dir, entry))
    return sorted(files)


def load_training_samples(data_dir: str, limit_samples: int = 0) -> Tuple[List[GameState], List[np.ndarray], List[float]]:
    npz_files = discover_npz_files(data_dir)
    if not npz_files:
        raise FileNotFoundError(f"No .npz files found in {data_dir}")

    all_game_states: List[GameState] = []
    all_policies: List[np.ndarray] = []
    all_values: List[float] = []

    for path in npz_files:
        with np.load(path, allow_pickle=True) as data:
            if 'states' not in data or 'policies' not in data or 'values' not in data:
                continue
            states_arr = data['states']
            policies_arr = data['policies']
            values_arr = data['values']

            # Decode json bytes to strings
            json_strings = []
            for elem in states_arr:
                if isinstance(elem, (bytes, bytearray)):
                    json_strings.append(elem.decode('utf-8'))
                else:
                    json_strings.append(str(elem))

            for i in range(min(len(json_strings), len(policies_arr), len(values_arr))):
                try:
                    gs = rehydrate_state(json_strings[i])
                    all_game_states.append(gs)
                    all_policies.append(np.asarray(policies_arr[i], dtype=np.float32))
                    all_values.append(float(values_arr[i]))
                except Exception:
                    # Skip malformed entries
                    continue

                if limit_samples and len(all_game_states) >= limit_samples:
                    break
        if limit_samples and len(all_game_states) >= limit_samples:
            break

    if not all_game_states:
        raise RuntimeError(f"No valid samples could be loaded from {data_dir}")

    return all_game_states, all_policies, all_values


def infer_default_output_path(data_dir: str) -> str:
    # Try to infer generation number from path: .../generation_0/data
    parent = os.path.dirname(os.path.abspath(data_dir))
    gen_name = os.path.basename(parent)
    gen_suffix = gen_name.replace('generation_', '') if gen_name.startswith('generation_') else 'x'
    models_dir = os.path.join(os.path.dirname(parent), 'models')
    os.makedirs(models_dir, exist_ok=True)
    return os.path.join(models_dir, f"pq_gen{gen_suffix}.keras")


def main():
    args = parse_args()

    if not TENSORFLOW_AVAILABLE:
        raise ImportError("TensorFlow is required for training PQNetwork. Please install tensorflow.")

    data_dir = os.path.abspath(args.data_dir)
    print(f"Loading data from: {data_dir}")
    game_states, policies, values = load_training_samples(data_dir, limit_samples=args.limit_samples)
    print(f"Loaded samples: {len(game_states)}")

    # Build training tuples for PQNetwork
    training_data = list(zip(game_states, policies, values))

    # Initialize PQNetwork
    pq = PQNetwork(hidden_size=args.hidden_size, embedding_dim=args.embedding_dim, max_turns=args.max_turns)
    print("Training PQNetwork...")
    pq.train(training_data, epochs=args.epochs, batch_size=args.batch_size)

    output_path = args.output or infer_default_output_path(data_dir)
    if not output_path.endswith('.keras'):
        output_path += '.keras'
    pq.save_model(output_path)
    print(f"Model saved to: {output_path}")


if __name__ == "__main__":
    main()


